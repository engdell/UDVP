# üöÄ Unstructured Data Vectorization Pipeline (UDVP)

A fully managed, continuous data pipeline in Snowflake that automatically ingests unstructured documents, extracts content, classifies them, chunks text, and generates vector embeddings for semantic search and RAG applications.

## üìã Overview

This pipeline leverages Snowflake's native features to provide a declarative, continuous ETL/ELT process for unstructured data:

- **Modern Cortex AI Functions**: `AI_PARSE_DOCUMENT`, `AI_CLASSIFY`, `AI_EMBED`
- **Dynamic Tables**: Automatic refresh and dependency management
- **Streams**: Change data capture for continuous processing
- **Internal Stages**: Secure document storage within Snowflake
- **Vector Similarity Search**: Semantic search using vector embeddings

## üèóÔ∏è Architecture

```
Unstructured Data    ‚Üí    Stream    ‚Üí    AI Parse     ‚Üí    AI Classify    ‚Üí    Chunking    ‚Üí    Embeddings
    Stage                 (CDC)         Document                                                   (Dynamic Table)
     ‚Üì                                      ‚Üì                    ‚Üì                                       ‚Üì
Internal Stage        Directory        Parsed Text         Classification           Text Chunks        Vector DB
                       Table                                                                          (Search Ready)
```

## üìÅ Project Structure

```
UDVP project/
‚îú‚îÄ‚îÄ .snowflake/
‚îÇ   ‚îî‚îÄ‚îÄ config                          # Snowflake CLI configuration
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ 01_setup_infrastructure.sql     # Database, schema, warehouse
‚îÇ   ‚îú‚îÄ‚îÄ 02_create_stage_and_directory.sql  # Internal stage setup
‚îÇ   ‚îú‚îÄ‚îÄ 03_create_chunking_udf.sql      # (Deprecated - using page_split)
‚îÇ   ‚îú‚îÄ‚îÄ 04_create_stream.sql            # CDC stream on directory table
‚îÇ   ‚îú‚îÄ‚îÄ 05_create_parsed_documents_dynamic_table.sql  # Parse & classify with page_split
‚îÇ   ‚îú‚îÄ‚îÄ 06_create_doc_embeddings_dynamic_table.sql    # Page-based embeddings
‚îÇ   ‚îú‚îÄ‚îÄ 07_create_cortex_search_service.sql  # Semantic search service
‚îÇ   ‚îú‚îÄ‚îÄ 08_monitoring_and_observability.sql  # Health monitoring views
‚îÇ   ‚îî‚îÄ‚îÄ 09_sample_queries.sql           # Example usage queries
‚îú‚îÄ‚îÄ deploy.sh                            # Deployment script
‚îú‚îÄ‚îÄ snowflake.yml                        # Snowflake project definition
‚îî‚îÄ‚îÄ README.md                            # This file
```

## üöÄ Quick Start

### Prerequisites

1. **Snowflake CLI**: Install from [Snowflake Documentation](https://docs.snowflake.com/en/developer-guide/snowflake-cli/index)
2. **Snowflake Account**: With Cortex AI features enabled
3. **Authentication**: `.pat` file with your Snowflake private key

### Installation

1. **Clone or navigate to the project directory**:
   ```bash
   cd /path/to/udvp
   ```

2. **Configure authentication**:
   - Ensure your `.pat` file contains your Snowflake private key
   - The `.snowflake/config` file is already configured to use the "cursor" connection

3. **Deploy the pipeline**:
   ```bash
   chmod +x deploy.sh
   ./deploy.sh
   ```

## üì§ Usage

### 1. Upload Documents

Upload documents to the internal stage:

```bash
# Single file
snow sql -q "PUT file:///path/to/document.pdf @UDVP_DB.UDVP_SCHEMA.RAW_DOCUMENTS_STAGE" -c cursor

# Multiple files
snow sql -q "PUT file:///path/to/documents/* @UDVP_DB.UDVP_SCHEMA.RAW_DOCUMENTS_STAGE" -c cursor
```

Supported formats: PDF, DOCX, TXT, and other formats supported by `PARSE_DOCUMENT`.

### 2. Refresh Stage Directory

After uploading files, refresh the directory table:

```bash
snow sql -q "ALTER STAGE UDVP_DB.UDVP_SCHEMA.RAW_DOCUMENTS_STAGE REFRESH" -c cursor
```

### 3. Monitor Pipeline

Check pipeline health:

```bash
snow sql -q "SELECT * FROM UDVP_DB.UDVP_SCHEMA.PIPELINE_HEALTH" -c cursor
```

View processing latency:

```bash
snow sql -q "SELECT * FROM UDVP_DB.UDVP_SCHEMA.PROCESSING_LATENCY" -c cursor
```

### 4. Query Documents

View parsed documents:

```sql
SELECT 
    FILE_PATH,
    CLASSIFICATION,
    LEFT(PARSED_TEXT, 200) AS PREVIEW
FROM UDVP_DB.UDVP_SCHEMA.PARSED_DOCUMENTS;
```

View embeddings:

```sql
SELECT 
    FILE_PATH,
    CLASSIFICATION,
    CHUNK_ID,
    TEXT_CHUNK
FROM UDVP_DB.UDVP_SCHEMA.DOC_EMBEDDINGS
LIMIT 100;
```

### 5. Semantic Search

Perform semantic search using vector similarity:

```sql
WITH search_vector AS (
    SELECT AI_EMBED('e5-base-v2', 'your search query here') AS query_embedding
)
SELECT 
    d.FILE_PATH,
    d.CLASSIFICATION,
    d.TEXT_CHUNK,
    d.CHUNK_ID,
    VECTOR_COSINE_SIMILARITY(d.VECTOR_EMBEDDING, s.query_embedding) AS SIMILARITY_SCORE
FROM UDVP_DB.UDVP_SCHEMA.DOC_EMBEDDINGS d
CROSS JOIN search_vector s
WHERE d.VECTOR_EMBEDDING IS NOT NULL
ORDER BY SIMILARITY_SCORE DESC
LIMIT 10;
```

## üîß Configuration

### Dynamic Table Refresh Frequency

Adjust the `TARGET_LAG` to balance freshness with compute costs:

```sql
-- Faster refresh (more compute)
ALTER DYNAMIC TABLE UDVP_DB.UDVP_SCHEMA.DOC_EMBEDDINGS 
    SET TARGET_LAG = '1 minute';

-- Slower refresh (less compute)
ALTER DYNAMIC TABLE UDVP_DB.UDVP_SCHEMA.DOC_EMBEDDINGS 
    SET TARGET_LAG = '15 minutes';
```

### Page Split Configuration

The pipeline uses `AI_PARSE_DOCUMENT` with the `page_split` option for automatic chunking by page. This works for PDF, DOCX, and PPTX files. For other formats or custom chunking needs, you can modify `05_create_parsed_documents_dynamic_table.sql`:

```sql
-- Current: {'mode': 'LAYOUT', 'page_split': true}

-- Without page splitting (single document)
{'mode': 'LAYOUT', 'page_split': false}

-- Process specific page ranges
{'mode': 'LAYOUT', 'page_filter': [{'start': 0, 'end': 10}]}
```

## üìä Monitoring Views

| View Name | Description |
|-----------|-------------|
| `PIPELINE_HEALTH` | Overall pipeline metrics and document counts |
| `FAILED_DOCUMENTS` | Documents that failed to parse |
| `PROCESSING_LATENCY` | Time taken for each processing stage |
| `DOCUMENT_STATISTICS` | Statistics by document classification |

## üéØ Key Features

- ‚úÖ **Fully Managed**: Leverages Snowflake's native services
- ‚úÖ **Continuous Processing**: Automatic processing of new documents
- ‚úÖ **Scalable**: Automatic compute scaling via Snowflake
- ‚úÖ **AI-Powered**: Uses modern Cortex AI functions (`AI_PARSE_DOCUMENT` with `page_split`, `AI_CLASSIFY`, `AI_EMBED`)
- ‚úÖ **Intelligent Chunking**: Built-in page-level chunking for PDFs, DOCX, and PPTX
- ‚úÖ **Search Ready**: Vector similarity search for semantic retrieval
- ‚úÖ **Monitored**: Built-in observability and health checks

## üìù Schema

### DOC_EMBEDDINGS Table

| Column | Type | Description |
|--------|------|-------------|
| `FILE_PATH` | VARCHAR | Original document path in stage |
| `DOCUMENT_ID` | VARCHAR | Unique document identifier (MD5 hash) |
| `CLASSIFICATION` | VARCHAR | AI-generated document classification |
| `CHUNK_ID` | INTEGER | Chunk index within document |
| `TEXT_CHUNK` | VARCHAR | Text segment |
| `VECTOR_EMBEDDING` | VECTOR(FLOAT, 768) | Embedding vector for semantic search |
| `PROCESSED_AT` | TIMESTAMP | When document was parsed |
| `EMBEDDING_GENERATED_AT` | TIMESTAMP | When embedding was generated |

## üîí Security

- **RBAC**: Uses role-based access control (SYSADMIN role)
- **Internal Stage**: Documents stored securely within Snowflake
- **JWT Authentication**: Uses private key authentication via `.pat` file

## üõ†Ô∏è Troubleshooting

### Documents not processing

1. Verify files are uploaded:
   ```sql
   SELECT * FROM DIRECTORY(@UDVP_DB.UDVP_SCHEMA.RAW_DOCUMENTS_STAGE);
   ```

2. Check for failed documents:
   ```sql
   SELECT * FROM UDVP_DB.UDVP_SCHEMA.FAILED_DOCUMENTS;
   ```

3. Manually refresh the stage:
   ```sql
   ALTER STAGE UDVP_DB.UDVP_SCHEMA.RAW_DOCUMENTS_STAGE REFRESH;
   ```

### Dynamic tables not refreshing

1. Check dynamic table status:
   ```sql
   SHOW DYNAMIC TABLES IN SCHEMA UDVP_DB.UDVP_SCHEMA;
   ```

2. Manually refresh:
   ```sql
   ALTER DYNAMIC TABLE UDVP_DB.UDVP_SCHEMA.DOC_EMBEDDINGS REFRESH;
   ```

### Embeddings are NULL

- Ensure text chunks are not empty
- Verify Cortex AI functions are available in your Snowflake account
- Check query history for errors

## üìö Resources

- [Snowflake Documentation](https://docs.snowflake.com/)
- [AI_PARSE_DOCUMENT](https://docs.snowflake.com/en/sql-reference/functions/ai_parse_document)
- [AI_CLASSIFY](https://docs.snowflake.com/en/sql-reference/functions/ai_classify)
- [AI_EMBED](https://docs.snowflake.com/en/sql-reference/functions/ai_embed)
- [Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)
- [Vector Data Type](https://docs.snowflake.com/en/sql-reference/data-types-vector)

## üìÑ License

This project is provided as-is for use with Snowflake Data Cloud.

## ü§ù Support

For issues or questions, refer to the Snowflake documentation or your Snowflake account team.

